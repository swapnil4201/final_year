{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "data = pd.read_csv(\"Boston_Housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        B  LSTAT  MEDV  Unnamed: 14  \n",
       "0  396.90   4.98  24.0          NaN  \n",
       "1  396.90   9.14  21.6          NaN  \n",
       "2  392.83   4.03  34.7          NaN  \n",
       "3  394.63   2.94  33.4          NaN  \n",
       "4  396.90   5.33  36.2          NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess the data\n",
    "X = data.drop(columns=['LSTAT'])\n",
    "y = data['LSTAT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(columns=['Unnamed: 14'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows NaN values\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess data again\n",
    "X = data.drop(columns=['MEDV'])#features\n",
    "y = data['MEDV']#target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the model\n",
    "model = Sequential([\n",
    "    Dense(128, activation=LeakyReLU(alpha=0.1), input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation=LeakyReLU(alpha=0.1)),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation=LeakyReLU(alpha=0.1)),\n",
    "    Dropout(0,2),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "11/11 [==============================] - 1s 17ms/step - loss: 18.7959 - val_loss: 17.2307\n",
      "Epoch 2/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.6609 - val_loss: 17.1466\n",
      "Epoch 3/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 18.3330 - val_loss: 17.1499\n",
      "Epoch 4/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 24.4705 - val_loss: 17.0276\n",
      "Epoch 5/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 22.4243 - val_loss: 17.0804\n",
      "Epoch 6/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.8864 - val_loss: 17.1435\n",
      "Epoch 7/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 22.4339 - val_loss: 17.1506\n",
      "Epoch 8/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.2574 - val_loss: 17.1161\n",
      "Epoch 9/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.0238 - val_loss: 17.2653\n",
      "Epoch 10/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.7882 - val_loss: 17.1787\n",
      "Epoch 11/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.8262 - val_loss: 17.0470\n",
      "Epoch 12/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.7597 - val_loss: 17.0681\n",
      "Epoch 13/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.1520 - val_loss: 17.1432\n",
      "Epoch 14/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.0241 - val_loss: 17.1646\n",
      "Epoch 15/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.9586 - val_loss: 17.2201\n",
      "Epoch 16/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.8017 - val_loss: 17.2159\n",
      "Epoch 17/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 20.9383 - val_loss: 17.2073\n",
      "Epoch 18/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.9175 - val_loss: 17.1577\n",
      "Epoch 19/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.6953 - val_loss: 17.1865\n",
      "Epoch 20/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.3460 - val_loss: 17.3483\n",
      "Epoch 21/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.7069 - val_loss: 17.4208\n",
      "Epoch 22/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 22.7686 - val_loss: 17.3379\n",
      "Epoch 23/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.1888 - val_loss: 16.9618\n",
      "Epoch 24/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.1022 - val_loss: 16.8510\n",
      "Epoch 25/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 21.0666 - val_loss: 16.7601\n",
      "Epoch 26/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.8658 - val_loss: 16.7150\n",
      "Epoch 27/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.5999 - val_loss: 16.8126\n",
      "Epoch 28/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.9484 - val_loss: 17.1249\n",
      "Epoch 29/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.4276 - val_loss: 17.0079\n",
      "Epoch 30/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.6434 - val_loss: 16.8671\n",
      "Epoch 31/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 19.3613 - val_loss: 16.7235\n",
      "Epoch 32/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.1672 - val_loss: 16.5099\n",
      "Epoch 33/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 21.6666 - val_loss: 16.3478\n",
      "Epoch 34/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.2298 - val_loss: 16.0498\n",
      "Epoch 35/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.2686 - val_loss: 16.0136\n",
      "Epoch 36/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 20.1483 - val_loss: 16.0435\n",
      "Epoch 37/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.1796 - val_loss: 16.0769\n",
      "Epoch 38/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.4118 - val_loss: 16.2275\n",
      "Epoch 39/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.1977 - val_loss: 16.4681\n",
      "Epoch 40/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 21.2843 - val_loss: 16.5423\n",
      "Epoch 41/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 21.0218 - val_loss: 16.5254\n",
      "Epoch 42/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 21.5253 - val_loss: 16.5782\n",
      "Epoch 43/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.5613 - val_loss: 16.5984\n",
      "Epoch 44/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 20.3149 - val_loss: 16.5597\n",
      "Epoch 45/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 22.3331 - val_loss: 16.7315\n",
      "Epoch 46/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.3792 - val_loss: 16.7656\n",
      "Epoch 47/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 19.3163 - val_loss: 16.6312\n",
      "Epoch 48/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 20.5859 - val_loss: 16.5518\n",
      "Epoch 49/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.5934 - val_loss: 16.4231\n",
      "Epoch 50/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.9471 - val_loss: 16.4485\n",
      "Epoch 51/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 21.1339 - val_loss: 16.5347\n",
      "Epoch 52/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.0787 - val_loss: 16.6925\n",
      "Epoch 53/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.5707 - val_loss: 16.8351\n",
      "Epoch 54/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.0199 - val_loss: 16.5049\n",
      "Epoch 55/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.8805 - val_loss: 16.2829\n",
      "Epoch 56/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.8819 - val_loss: 16.2592\n",
      "Epoch 57/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.2716 - val_loss: 16.2751\n",
      "Epoch 58/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 21.2908 - val_loss: 16.0608\n",
      "Epoch 59/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.1232 - val_loss: 16.1901\n",
      "Epoch 60/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 19.4529 - val_loss: 16.5792\n",
      "Epoch 61/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.3351 - val_loss: 16.5902\n",
      "Epoch 62/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.5340 - val_loss: 16.5484\n",
      "Epoch 63/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.9870 - val_loss: 16.4670\n",
      "Epoch 64/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.8080 - val_loss: 16.1836\n",
      "Epoch 65/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.7542 - val_loss: 16.2003\n",
      "Epoch 66/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.7245 - val_loss: 16.3466\n",
      "Epoch 67/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.5441 - val_loss: 16.3591\n",
      "Epoch 68/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 18.4623 - val_loss: 16.2738\n",
      "Epoch 69/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.2314 - val_loss: 16.1275\n",
      "Epoch 70/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 19.6824 - val_loss: 15.9864\n",
      "Epoch 71/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.3138 - val_loss: 15.8655\n",
      "Epoch 72/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.2428 - val_loss: 15.8177\n",
      "Epoch 73/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.5364 - val_loss: 15.7567\n",
      "Epoch 74/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.8064 - val_loss: 15.7553\n",
      "Epoch 75/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.7097 - val_loss: 15.5294\n",
      "Epoch 76/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.2591 - val_loss: 15.8002\n",
      "Epoch 77/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.6466 - val_loss: 16.1332\n",
      "Epoch 78/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.7675 - val_loss: 16.1547\n",
      "Epoch 79/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.8618 - val_loss: 15.8469\n",
      "Epoch 80/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.6806 - val_loss: 15.6728\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 0s 4ms/step - loss: 16.6441 - val_loss: 15.6948\n",
      "Epoch 82/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 21.0555 - val_loss: 15.6845\n",
      "Epoch 83/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.8773 - val_loss: 15.7845\n",
      "Epoch 84/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.9125 - val_loss: 15.7327\n",
      "Epoch 85/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.1994 - val_loss: 15.5949\n",
      "Epoch 86/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.3715 - val_loss: 15.4833\n",
      "Epoch 87/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.4852 - val_loss: 15.4930\n",
      "Epoch 88/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.4951 - val_loss: 15.6442\n",
      "Epoch 89/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.1806 - val_loss: 15.7140\n",
      "Epoch 90/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.4860 - val_loss: 15.9193\n",
      "Epoch 91/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.5294 - val_loss: 15.9455\n",
      "Epoch 92/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 15.8260 - val_loss: 15.7215\n",
      "Epoch 93/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.5871 - val_loss: 15.6066\n",
      "Epoch 94/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.7573 - val_loss: 15.4488\n",
      "Epoch 95/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.8730 - val_loss: 15.2147\n",
      "Epoch 96/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.6165 - val_loss: 15.1116\n",
      "Epoch 97/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.0516 - val_loss: 14.9367\n",
      "Epoch 98/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.2611 - val_loss: 14.7335\n",
      "Epoch 99/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.4524 - val_loss: 14.9502\n",
      "Epoch 100/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.3526 - val_loss: 15.0533\n",
      "Epoch 101/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.8380 - val_loss: 15.0441\n",
      "Epoch 102/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.0520 - val_loss: 14.9733\n",
      "Epoch 103/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.4412 - val_loss: 15.2197\n",
      "Epoch 104/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.2830 - val_loss: 15.4596\n",
      "Epoch 105/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.2356 - val_loss: 15.4852\n",
      "Epoch 106/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.7909 - val_loss: 15.9030\n",
      "Epoch 107/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.1527 - val_loss: 16.0644\n",
      "Epoch 108/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.8928 - val_loss: 15.7386\n",
      "Epoch 109/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.2209 - val_loss: 15.5196\n",
      "Epoch 110/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 16.2113 - val_loss: 15.4711\n",
      "Epoch 111/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.8202 - val_loss: 15.3720\n",
      "Epoch 112/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.1129 - val_loss: 15.3486\n",
      "Epoch 113/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.2213 - val_loss: 15.2737\n",
      "Epoch 114/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.7105 - val_loss: 15.2730\n",
      "Epoch 115/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.8844 - val_loss: 15.2020\n",
      "Epoch 116/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.7671 - val_loss: 15.3333\n",
      "Epoch 117/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.7306 - val_loss: 15.4366\n",
      "Epoch 118/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.6413 - val_loss: 15.3648\n",
      "Epoch 119/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.9621 - val_loss: 15.2395\n",
      "Epoch 120/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.4355 - val_loss: 15.3660\n",
      "Epoch 121/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.2801 - val_loss: 15.4146\n",
      "Epoch 122/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.8688 - val_loss: 15.4778\n",
      "Epoch 123/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.5179 - val_loss: 15.5078\n",
      "Epoch 124/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.6636 - val_loss: 15.2643\n",
      "Epoch 125/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 20.0044 - val_loss: 15.1587\n",
      "Epoch 126/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.7252 - val_loss: 15.2086\n",
      "Epoch 127/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.3170 - val_loss: 15.1316\n",
      "Epoch 128/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.0100 - val_loss: 14.9857\n",
      "Epoch 129/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 14.4455 - val_loss: 15.1566\n",
      "Epoch 130/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.8005 - val_loss: 15.0956\n",
      "Epoch 131/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 18.7988 - val_loss: 14.9781\n",
      "Epoch 132/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.1919 - val_loss: 14.9771\n",
      "Epoch 133/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.3199 - val_loss: 14.9326\n",
      "Epoch 134/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.6357 - val_loss: 14.8363\n",
      "Epoch 135/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.7748 - val_loss: 14.8928\n",
      "Epoch 136/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 15.9081 - val_loss: 14.8614\n",
      "Epoch 137/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.2614 - val_loss: 14.7956\n",
      "Epoch 138/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.0316 - val_loss: 14.8102\n",
      "Epoch 139/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.1592 - val_loss: 14.8581\n",
      "Epoch 140/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.6470 - val_loss: 14.8255\n",
      "Epoch 141/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.4853 - val_loss: 14.7246\n",
      "Epoch 142/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.5355 - val_loss: 14.9526\n",
      "Epoch 143/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.6757 - val_loss: 14.9548\n",
      "Epoch 144/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.2288 - val_loss: 14.9801\n",
      "Epoch 145/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.4518 - val_loss: 14.8695\n",
      "Epoch 146/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.3494 - val_loss: 14.7580\n",
      "Epoch 147/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.6366 - val_loss: 14.7128\n",
      "Epoch 148/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.4191 - val_loss: 14.7573\n",
      "Epoch 149/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.8109 - val_loss: 14.7243\n",
      "Epoch 150/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.3355 - val_loss: 14.6850\n",
      "Epoch 151/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.9418 - val_loss: 14.5819\n",
      "Epoch 152/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 17.2539 - val_loss: 14.3270\n",
      "Epoch 153/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.9285 - val_loss: 14.3088\n",
      "Epoch 154/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.1054 - val_loss: 14.4279\n",
      "Epoch 155/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 12.9145 - val_loss: 14.5957\n",
      "Epoch 156/200\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 17.9715 - val_loss: 14.5790\n",
      "Epoch 157/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.1470 - val_loss: 15.0870\n",
      "Epoch 158/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.7285 - val_loss: 15.5923\n",
      "Epoch 159/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.9442 - val_loss: 15.7081\n",
      "Epoch 160/200\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 17.2580 - val_loss: 15.5888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 15.5491 - val_loss: 15.4594\n",
      "Epoch 162/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 14.8052 - val_loss: 15.3455\n",
      "Epoch 163/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.2505 - val_loss: 15.0399\n",
      "Epoch 164/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.6483 - val_loss: 14.7607\n",
      "Epoch 165/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.9265 - val_loss: 14.8015\n",
      "Epoch 166/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 15.8452 - val_loss: 14.8818\n",
      "Epoch 167/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 14.0522 - val_loss: 14.7913\n",
      "Epoch 168/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.1484 - val_loss: 14.7153\n",
      "Epoch 169/200\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 17.8340 - val_loss: 14.8917\n",
      "Epoch 170/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.1662 - val_loss: 15.1021\n",
      "Epoch 171/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.7112 - val_loss: 15.0446\n",
      "Epoch 172/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 15.9759 - val_loss: 14.8313\n",
      "Epoch 173/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.4496 - val_loss: 14.6825\n",
      "Epoch 174/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.5898 - val_loss: 14.6031\n",
      "Epoch 175/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.0505 - val_loss: 14.7862\n",
      "Epoch 176/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 14.5388 - val_loss: 14.7192\n",
      "Epoch 177/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.6604 - val_loss: 14.9415\n",
      "Epoch 178/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.5173 - val_loss: 14.9190\n",
      "Epoch 179/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.2533 - val_loss: 14.6915\n",
      "Epoch 180/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.1478 - val_loss: 14.5763\n",
      "Epoch 181/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.2419 - val_loss: 14.4326\n",
      "Epoch 182/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 15.5846 - val_loss: 14.4378\n",
      "Epoch 183/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 18.7169 - val_loss: 14.4689\n",
      "Epoch 184/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 19.9257 - val_loss: 14.4957\n",
      "Epoch 185/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.4214 - val_loss: 14.5539\n",
      "Epoch 186/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 15.9747 - val_loss: 14.7299\n",
      "Epoch 187/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 15.1981 - val_loss: 14.7419\n",
      "Epoch 188/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.7728 - val_loss: 14.5971\n",
      "Epoch 189/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 15.9080 - val_loss: 14.5501\n",
      "Epoch 190/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.4410 - val_loss: 14.6383\n",
      "Epoch 191/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.9474 - val_loss: 14.6307\n",
      "Epoch 192/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 16.1223 - val_loss: 14.4631\n",
      "Epoch 193/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.4790 - val_loss: 14.4053\n",
      "Epoch 194/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.2277 - val_loss: 14.1438\n",
      "Epoch 195/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.1554 - val_loss: 14.0531\n",
      "Epoch 196/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.0834 - val_loss: 14.0990\n",
      "Epoch 197/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 15.7802 - val_loss: 14.2566\n",
      "Epoch 198/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 14.8800 - val_loss: 14.2203\n",
      "Epoch 199/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 15.5043 - val_loss: 14.2413\n",
      "Epoch 200/200\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 17.5153 - val_loss: 14.2637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x243e888e2f0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adjusted training parameters\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "model.fit(X_train_scaled, y_train, epochs=200, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 15.8829\n",
      "Test Loss: 15.882882118225098\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model\n",
    "test_loss = model.evaluate(X_test_scaled, y_test)\n",
    "print('Test Loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "#make prediction\n",
    "prediction = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some predictions and Actual values:\n",
      "Predicted Price: 28.158865 Actual Price: 23.6\n",
      "Predicted Price: 32.39315 Actual Price: 32.4\n",
      "Predicted Price: 19.5896 Actual Price: 13.6\n",
      "Predicted Price: 27.415195 Actual Price: 22.8\n",
      "Predicted Price: 16.045967 Actual Price: 16.1\n"
     ]
    }
   ],
   "source": [
    "#print some predictions and actual values\n",
    "print('Some predictions and Actual values:')\n",
    "for i in range(5):\n",
    "    print('Predicted Price:', prediction[i][0], 'Actual Price:', y_test.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
